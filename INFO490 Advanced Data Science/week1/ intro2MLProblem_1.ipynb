{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d15639e60bfab38d47579d58e81ac2b3",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 1 Problem 1\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select *Kernel*, and restart the kernel and run all cells (*Restart & Run all*).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select *File* → *Save and CheckPoint*)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to *Dashboard* → *Assignments* and click the *Submit* button. Your work is not submitted until you click *Submit*.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9c6cb422acb2c9905271ca8c9f80b5a",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, January 22, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2915aa91d5475c368216b4c867fabb6c",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal, assert_true\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Dataset\n",
    "For this assignment we will be using the built-in dataset about the Boston area and the respective house-prices. This dataset has 506 samples and a dimensionality size of 13. Each record contains data about crime rate, average number of rooms dwelling, and other factors. The following code below imports the dataset as a pandas dataframe and previews a few sample data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "96121e716804b2aed679784f7369b76e",
     "grade": false,
     "grade_id": "data_set",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NOTE: Make sure to load this data set before completing the assignment\n",
    "'''\n",
    "# Load in the dataset as a Pandas DataFrame\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "print(data.DESCR)\n",
    "\n",
    "# Print the dataset description\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Preview the first few lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Review Pandas\n",
    "\n",
    "The purpose of this notebook is to review and implement some of the introductory concepts in machine learning introduced in Week 1. In particular, this problem will review the Pandas library which is used for large scale data processing in Python. Complete the following function `get_top_k_crime_rates` that takes in 2 parameters: `crime_rate` and `k` that returns the top `k` records that have a crime rate over the `crime_rate` parameter. For example, given the first 5 records as the sample dataframe and the `crime_rate` threshold = 0.01 and the `k` value = 2. Then records 4 and 3 would be returned respectively since these are the top 2 records with a crime_rate threshold over 0.01.\n",
    "\n",
    "**NOTE: Filter the records by the CRIM attribute and the corresponding threshold in order to retrieve the records with crime rates higher than the threshold. Also make sure to cast the final object to a pd.DataFrame object as the autograder will be checking for this. The records should also be in descending order.**\n",
    "\n",
    "**HINT: A useful resource to sort values: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "386738f7f90b07f7c9822d0cc5078dc7",
     "grade": false,
     "grade_id": "problem1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_top_k_crime_rates(crime_rate, k):\n",
    "    '''\n",
    "    Return the top k records that are over the crime_rate threshold\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    crime_rate: A double that represents the crime_rate threshold\n",
    "    k: An integer that represents the number of records to return\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    crime_rate_values: A pandas.DataFrame that contains the top k records over the crime_rate threshold\n",
    "    '''\n",
    "    crime_rate_values = None\n",
    "    crime_rate_values = df[df['CRIM'] > crime_rate].sort_values(by = 'CRIM', ascending = False)[0:k]\n",
    "    return crime_rate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "880fb4657f0890c85f612f79848a078e",
     "grade": true,
     "grade_id": "problem1_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(isinstance(get_top_k_crime_rates(20.0, 2), pd.DataFrame))\n",
    "assert_equal(len(get_top_k_crime_rates(20.0, 2)), 2)\n",
    "assert_equal(get_top_k_crime_rates(20.0, 2).iloc[0]['CRIM'], 88.9762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Data Scaling\n",
    "\n",
    "In the next few problems, you will explore how to split your data between testing and training data using the `test_train_split` function in the sklean library. However, another important aspect of preprocessing data is to scale the data across all features accordingly so that one feature with a much larger span does not dominate the algorithm. \n",
    "\n",
    "As discussed in the lesson, there are a few various scaling methods. In this particular question, we will be implementing the range technique. If you look at the statistics for the various features, it is evident that all the features are not normally distributed. Some of the features like `B` have a higher max than many of the other features and the range is much larger.\n",
    "\n",
    "Complete the function `data_scale_range` that takes in 2 parameters: `d_train` and `d_test` which is the training and test data respectively and returns `d_train_scaled` and `d_test_scaled` (as a 2-tuple) which is the training and test data scaled respectively. You will need to use a `MinMaxScaler` in order to preprocess the data and apply the range scaling method on the data. **Resource: http://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "929094d334ab0073b30de079900d56ac",
     "grade": false,
     "grade_id": "problem2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def data_scale_range(d_train, d_test):\n",
    "    '''\n",
    "    Return the trained and testing data scaled from 0 to 1 using the range technique\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_train: A Pandas dataframe representing the training data\n",
    "    d_test: A Pandas dataframe represengint the testing data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d_train_scaled, d_test_scaled: A 2-tuple\n",
    "    numpy.ndArray that returns the training and testing data scaled respectively\n",
    "    '''\n",
    "    d_train_scaled, d_test_scaled = None, None\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    d_train_scaled = min_max_scaler.fit_transform(d_train)\n",
    "    d_test_scaled = min_max_scaler.transform(d_test)\n",
    "    \n",
    "    return d_train_scaled, d_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a41dbf93990061faff57a75f46f8d7b5",
     "grade": true,
     "grade_id": "problem2_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "d_train, d_test, _, _ = train_test_split(df, data.target, test_size=0.3, random_state=23)\n",
    "d_train_scaled, d_test_scaled = data_scale_range(d_train, d_test)\n",
    "assert_equal(len(d_train), len(d_train_scaled))\n",
    "assert_equal(len(d_test), len(d_test_scaled))\n",
    "for val, val_sc in zip(d_train, d_train_scaled):\n",
    "    assert_true(val != val_sc)\n",
    "for val, val_sc in zip(d_test, d_test_scaled):\n",
    "    assert_true(val != val_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Dimensionality Reduction\n",
    "\n",
    "In this question, we will be performing a dimensionality reduction using PCA as shown in this week's lesson. The Boston dataset we have been using thus far has a dimensionality size of 13. Our goal in this question will be reduce the dimensionality from 13 to the number of components specified as a parameter (`num_components`) to the function `dimensionality_reduction()`.\n",
    "\n",
    "Complete the function `dimensionality_reduction()` that takes in two parameters `num_components` and `data`. Perform a Principal Component Analysis (PCA) on the `data` and return the transformed data. **Note: You do not need to convert the transformed data into a pandas DataFrame. Simply transforming the data is sufficient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b838b8b723c25e0c07c2389d9bd51e36",
     "grade": false,
     "grade_id": "problem3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dimensionality_reduction(num_components, data):\n",
    "    '''\n",
    "    Return the reduced data with a dimensionality size of num_components\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_components: An integer representing the reduced dimensionality size\n",
    "    data: A numpy.ndArray representing the data values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transformed_data: A numpy.ndArray representing the data with a reduced dimensionality size\n",
    "    '''\n",
    "    transformed_data = None\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    # define number of components\n",
    "    pca = PCA(n_components=num_components)\n",
    "\n",
    "    # Fit model to the data\n",
    "    pca.fit(data)\n",
    "\n",
    "    # Compute the transformed data (rotation to PCA space)\n",
    "    transformed_data = pca.transform(data)\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "32ec8f8b569a4ef63dc07fe42fe89009",
     "grade": true,
     "grade_id": "problem3_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reduced_data = dimensionality_reduction(6, df.values)\n",
    "for data in reduced_data:\n",
    "    assert_true(len(data) == 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Model Persistence\n",
    "Complete the function `persist_PCA_model` below that takes in 1 parameter: `filename`, and creates a PCA model with number of components set to **6** and write this PCA model to the specified `filename`. Make sure to use the joblib library in order to write the PCA model to the specified filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "8b2274a7e2d963b7e3ac5d7b9abf366a",
     "grade": false,
     "grade_id": "problem4_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def persist_PCA_model(filename):\n",
    "    '''\n",
    "    Create a PCA model of dimensionality size 6 and write this model to the specified filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: A string describing the filename to write the PCA model to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    pca = PCA(n_components=6)\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    with open(filename, 'wb') as fout:\n",
    "        joblib.dump(pca, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52644a512f4005020889a3dcdfe9b137",
     "grade": true,
     "grade_id": "problem4_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "persist_PCA_model('test_model.pkl')\n",
    "assert_true(os.path.exists('test_model.pkl'))\n",
    "!rm test_model.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
