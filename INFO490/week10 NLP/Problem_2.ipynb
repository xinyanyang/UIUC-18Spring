{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1cf079d84ec8efc2b2a07a365462413",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 10 Problem 2\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says YOUR CODE HERE. Do not write your answer in anywhere else other than where it says YOUR CODE HERE. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select Kernel, and restart the kernel and run all cells (Restart & Run all).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select File → Save and CheckPoint)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to Dashboard → Assignments and click the Submit button. Your work is not submitted until you click Submit.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "7. If your code does not pass the unit tests, it will not pass the autograder.\n",
    "\n",
    "**Note:** **Start early since Validation will take some time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "814ff1da7485a13bc060abd7e3cca6b1",
     "grade": false,
     "grade_id": "names",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Author: Apurv Garg\n",
    "### Primary Reviewer: Radhir Kothuri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "443aa0aeddaedb7d457f924b9de7c21a",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, April 02, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf97f768eca7c4323525ba8a879ca192",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim import models\n",
    "from gensim import matutils as mat\n",
    "from gensim import models as md\n",
    "from gensim import corpora\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nose.tools import assert_equal, assert_true, assert_false, assert_almost_equal\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dd8ffc973e4b12ef3c6bb54a708c7ade",
     "grade": false,
     "grade_id": "cell-013a84948bcced6d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_topics(cv, model):\n",
    "    # Number of terms per topic to display\n",
    "    max_topics = 10\n",
    "\n",
    "    # Number of terms per topic to retain\n",
    "    max_labels = 5\n",
    "\n",
    "    topics = []\n",
    "    feature_names = cv.get_feature_names()\n",
    "\n",
    "    # Iterate through the matrix components\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "\n",
    "        # First we sort the terms in descending order ([::-1])\n",
    "        # And then retiain only the top terms\n",
    "        top_topics_idx = topic.argsort()[::-1][:max_topics]\n",
    "\n",
    "        top_topics = [feature_names[jdx] for jdx in top_topics_idx]\n",
    "\n",
    "        # Now extract out the terms themselves and display\n",
    "        top_features = \" \".join(top_topics)\n",
    "        # print('Topic {0:2d}: {1}'.format(idx, top_features))\n",
    "        topics.append(\", \".join(top_topics[:max_labels]))\n",
    "        \n",
    "    return(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1d0020d7f36ded9454c331697955215b",
     "grade": false,
     "grade_id": "dataset",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Dataset\n",
    "\n",
    "We will analyze the twenty newsgroup data set. We have removed the headers, quotes and footers. We will be analyzing four-grams only for the 1st two problems. \n",
    "\n",
    "The cell below will create a subdirectory under home called `temp_data`. *If you want to the delete the temp_data directory at any point, run this code in a new cell.*  \n",
    "``` bash\n",
    "! rm -rf /home/data_scientist/temp_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "509661241fe9c4db351261e39dae4f68",
     "grade": false,
     "grade_id": "cell-136aaf5f5e5008c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/data_scientist/temp_data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/temp_data\n",
    "HOME = '/home/data_scientist/temp_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d547816ae14a52a82663f4776cf0532",
     "grade": false,
     "grade_id": "cell-0e534d49498386ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(HOME, subset='train', remove =('quotes', 'headers', 'footers'), random_state=40)\n",
    "test = fetch_20newsgroups(HOME, subset='train', remove =('quotes', 'headers', 'footers'), random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25a772a307d763ea6056ed0fa2e599cf",
     "grade": false,
     "grade_id": "tfidf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Use TD-IDF on newgroup data.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#We are taking four-grams only\n",
    "cv = TfidfVectorizer(stop_words = 'english',ngram_range = (4,4),\n",
    "                     lowercase=True,\n",
    "                     min_df=2,\n",
    "                     max_features=1500)\n",
    "                     \n",
    "train_data = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "36dbb20593600010690346fb9be034ab",
     "grade": false,
     "grade_id": "problem1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "For this problem, complete the function `nmf_rf` which will take `num_topics`, `cv`, `train_data`, `test_data` and `target` as parameters and return the nmf_topics(data clusters after applying get_topics), td_norm(train dataset after transformation and l1 normalization), rfc(RF model after fitting it to train) and ts_preds(predictions for the test dataset using this model)\n",
    "\n",
    "- Apply non-negative matrix factorization(NMF) to compute topics in a corpus. The parameters to be used inside the NMF are `n_components`=num_topics and `max_iter`=5. Other parameters should be kept as default. Fit this on train_dataset.\n",
    "- Identify data clusters (or topics) in the corpus using get_topics.\n",
    "- Transform the dataset and normalize by using l1-norm. \n",
    "- Create a with RandomForestClassifier with parameters: `max_features='auto', min_samples_split=4, random_state=23`. Other parameters are left to be default. <br>\n",
    "- Fit estimator to scaled training data(`target` is the response variable) and predict for the test dataset using this model. (Don't forget to apply nmf transformation to test dataset while/before prediction) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "22349a4c97b9ba80de2ec500f35ee5e4",
     "grade": false,
     "grade_id": "problem1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def nmf_rf(num_topics, cv, train_dataset, test_dataset, target):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_topics : Number of topics\n",
    "    cv : TF-IDF vectorizer Object\n",
    "    train_dataset : transformed train-data after TF-IDF vectorization\n",
    "    test_dataset : transformed test-data after TF-IDF vectorization\n",
    "    target: target/dependent variable to be used in RF\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 4 containing nmf_topics(data clusters after applying get_topics), \n",
    "    td_norm(train dataset after transformation and l1 normalization), \n",
    "    rfc(RF model after fitting it to train) and \n",
    "    ts_preds(predictions for the test dataset using this model)\n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    nmf = NMF(n_components = num_topics, max_iter = 5).fit(train_dataset)\n",
    "    nmf_topics = get_topics(cv, nmf)\n",
    "    \n",
    "    td = nmf.transform(train_dataset)\n",
    "    td_norm = normalize(td, norm='l1', axis=1)\n",
    "    \n",
    "    rf = RandomForestClassifier(max_features='auto', min_samples_split=4, random_state=23)\n",
    "    rfc = rf.fit(td_norm, target)\n",
    "    \n",
    "    tst_data = nmf.transform(test_dataset)\n",
    "    ts_preds = rfc.predict(tst_data)\n",
    "    \n",
    "    return nmf_topics, td_norm, rfc, ts_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "04182da0c80a323c66177a0ccc5c51aa",
     "grade": true,
     "grade_id": "problem1_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "nmf_topics, td_norm, rfc1, ts_preds = nmf_rf(30, cv, train_data, test_data, train['target'])\n",
    "assert_equal(isinstance(rfc1, RandomForestClassifier), True)\n",
    "assert_equal(len(nmf_topics), 30)\n",
    "assert_equal(rfc1.min_samples_split, 4)\n",
    "assert_equal(isinstance(ts_preds, np.ndarray), True)\n",
    "assert_equal(len(ts_preds), 11314)\n",
    "assert_equal(ts_preds[[1]][0], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fb0f622254c76abe9343058c67125b1",
     "grade": false,
     "grade_id": "cell-112c8c590b3077dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.93      0.05      0.10       480\n",
      "           comp.graphics       0.10      0.01      0.02       584\n",
      " comp.os.ms-windows.misc       0.59      0.02      0.03       591\n",
      "comp.sys.ibm.pc.hardware       0.00      0.00      0.00       590\n",
      "   comp.sys.mac.hardware       0.03      0.00      0.00       578\n",
      "          comp.windows.x       0.28      0.02      0.03       593\n",
      "            misc.forsale       0.15      0.01      0.03       585\n",
      "               rec.autos       0.00      0.00      0.00       594\n",
      "         rec.motorcycles       0.42      0.03      0.06       598\n",
      "      rec.sport.baseball       0.38      0.03      0.05       597\n",
      "        rec.sport.hockey       0.00      0.00      0.00       600\n",
      "               sci.crypt       0.77      0.07      0.13       595\n",
      "         sci.electronics       0.81      0.03      0.06       591\n",
      "                 sci.med       0.63      0.14      0.23       594\n",
      "               sci.space       0.30      0.07      0.12       593\n",
      "  soc.religion.christian       0.06      0.98      0.11       599\n",
      "      talk.politics.guns       0.74      0.07      0.12       546\n",
      "   talk.politics.mideast       1.00      0.09      0.17       564\n",
      "      talk.politics.misc       0.15      0.05      0.07       465\n",
      "      talk.religion.misc       0.00      0.00      0.00       377\n",
      "\n",
      "             avg / total       0.37      0.09      0.07     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test['target'], ts_preds,\n",
    "    target_names = test['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9dc32da00c1ac1837b9b2284bb7d49ad",
     "grade": false,
     "grade_id": "problem2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "For this problem, complete the function `lda_rf` which will take `num_topics`, `cv`, `train_data`, `test_data` and `target` as parameters and return the lda_topics(data clusters after applying get_topics), lda_train_norm(train dataset after transformation and l1 normalization), rfc(RF model after fitting it to train) and ts_preds(predictions for the test dataset using this model).\n",
    "\n",
    "- Apply Latent Dirichlet Allocation(LDA) to compute topics in a corpus. The parameters to be used inside the LDA are `n_topics`=num_topics, `max_iter`=5, `learning_method`='online',`learning_offset`=5 and `random_state`=23. Other parameters should be kept as default. Fit this on train_dataset.\n",
    "- Identify data clusters (or topics) in the corpus using get_topics.\n",
    "- Transform the dataset and normalize by using l1-norm. \n",
    "- Create a with RandomForestClassifier with parameters: `max_features='auto', min_samples_split=4, random_state=23`. Other parameters are left to be default. <br>\n",
    "- Fit estimator to scaled training data(`target` is the response variable) and predict for the test dataset using this model. (Don't forget to apply lda transformation to test dataset while/before prediction) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "11bbee5aa57381f98ab0b96b01b65a7a",
     "grade": false,
     "grade_id": "problem2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lda_rf(num_topics, cv, train_dataset, test_dataset, target):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_topics : Number of topics\n",
    "    cv : TF-IDF vectorizer Object\n",
    "    train_dataset : transformed train-data after TF-IDF vectorization\n",
    "    test_dataset : transformed test-data after TF-IDF vectorization\n",
    "    target: target/dependent variable to be used in RF\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 4 containing lda_topics(data clusters after applying get_topics), \n",
    "    lda_train_norm(train dataset after transformation and l1 normalization), \n",
    "    rfc(RF model after fitting it to train) and \n",
    "    ts_preds(predictions for the test dataset using this model)\n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=5, learning_method='online',\n",
    "                                    learning_offset=5, random_state=23).fit(train_dataset)\n",
    "    \n",
    "    lda_topics = get_topics(cv, lda)\n",
    "    \n",
    "    td = lda.transform(train_dataset)\n",
    "    lda_train_norm = normalize(td, norm='l1', axis=1)\n",
    "    \n",
    "    rf = RandomForestClassifier(max_features='auto', min_samples_split=4, random_state=23)\n",
    "    rfc = rf.fit(lda_train_norm, target)\n",
    "    \n",
    "    tst_data = lda.transform(test_dataset)\n",
    "    ts_preds = rfc.predict(tst_data)\n",
    "    \n",
    "    return lda_topics, lda_train_norm, rfc, ts_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "667cf43cde20aa793658ed2305c02993",
     "grade": true,
     "grade_id": "problem2_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lda_topics, lda_train_norm, rfc2, ts_preds2 = lda_rf(30, cv, train_data, test_data, train['target'])\n",
    "\n",
    "assert_equal(isinstance(rfc2, RandomForestClassifier), True)\n",
    "assert_equal(rfc2.min_samples_split, 4)\n",
    "assert_equal(isinstance(ts_preds2, np.ndarray), True)\n",
    "assert_equal(len(ts_preds2), 11314)\n",
    "assert_equal(ts_preds2[[1]][0], 15)\n",
    "assert_equal(ts_preds2[[0]][0], 6)\n",
    "assert_almost_equal(lda_train_norm[1][1], 0.033333333333333333, 3)\n",
    "assert_almost_equal(lda_train_norm[0][1], 0.016666666666667971, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aab118269c7de986be7742f69191962b",
     "grade": false,
     "grade_id": "cell-ec7f26e3e18d9e18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.88      0.09      0.16       480\n",
      "           comp.graphics       0.54      0.04      0.07       584\n",
      " comp.os.ms-windows.misc       0.67      0.08      0.14       591\n",
      "comp.sys.ibm.pc.hardware       0.82      0.02      0.05       590\n",
      "   comp.sys.mac.hardware       0.79      0.02      0.04       578\n",
      "          comp.windows.x       0.73      0.10      0.18       593\n",
      "            misc.forsale       0.75      0.03      0.06       585\n",
      "               rec.autos       0.75      0.02      0.04       594\n",
      "         rec.motorcycles       0.95      0.06      0.11       598\n",
      "      rec.sport.baseball       0.97      0.05      0.10       597\n",
      "        rec.sport.hockey       0.91      0.10      0.17       600\n",
      "               sci.crypt       0.90      0.13      0.23       595\n",
      "         sci.electronics       0.85      0.03      0.06       591\n",
      "                 sci.med       0.94      0.16      0.27       594\n",
      "               sci.space       0.95      0.09      0.16       593\n",
      "  soc.religion.christian       0.06      0.98      0.11       599\n",
      "      talk.politics.guns       0.70      0.09      0.15       546\n",
      "   talk.politics.mideast       0.93      0.16      0.27       564\n",
      "      talk.politics.misc       0.81      0.10      0.18       465\n",
      "      talk.religion.misc       0.33      0.01      0.02       377\n",
      "\n",
      "             avg / total       0.77      0.12      0.13     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test['target'], ts_preds2,\n",
    "    target_names = test['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5fd4f73429adc272e58613bb280af56",
     "grade": false,
     "grade_id": "cell-fece19bd5f1c43bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Dataset Creation(Problem3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e354251a69b5b715bd6fcf23024c8ed",
     "grade": false,
     "grade_id": "problem3_data",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "en_stop = set('my but for a of the and to in on an is that'.split())\n",
    "\n",
    "txts = [[word for word in sentance.lower().split() if word not in en_stop]\n",
    "        for sentance in doc_set]\n",
    "\n",
    "# Keep only those words appearing more than once\n",
    "from collections import Counter\n",
    "frequency = Counter([word for txt in txts for word in txt])\n",
    "\n",
    "# Now grab tokens that appear more than once\n",
    "tokens = [[token for token in txt if frequency[token] > 1]\n",
    "          for txt in txts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7e476c2eaa080570102e8102cbd1222a",
     "grade": false,
     "grade_id": "problem3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "For this problem, complete the function `gensim_tm` which will take `token`, `model` and `topics` as parameters and return the computed LDA or LSI model for corpus. \n",
    "- The parameters to be used inside the LdaModel/LsiModel are `corpus`, `id2word` and `num_topics`. Other parameters should be kept as default.\n",
    "- Compute a dictionary mapping for given text corpus(`id2word` parameter for model). <br>\n",
    "- Create corpus as bag of words(can use `doc2bow`) and convert text to TFIDF model(`corpus` parameter for model). <br>\n",
    "- Construct an Latent Dirichlet Allocation/ Latent Semantic Analysis model of this document using our dictionary mapping object.\n",
    "\n",
    "If model parameter equals 'lda', then return a LDA model and if model parameter equals 'lsi', return LSA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6398843c593bdb66ce5b21b6a5b145c4",
     "grade": false,
     "grade_id": "problem3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gensim_tm(token, model, topics):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    token : tokens for which dictionary mapping has to be created\n",
    "    model : the model which is to be ceated i.e. 'lda' or 'lsi'\n",
    "    topics : Number of requested factors(num_topics parameter for Lda/Lsi model) \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The computed LDA/LSI model for corpus    \n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    # Compute a dictionary mapping for given text corpus\n",
    "    dict_gensim = corpora.Dictionary(tokens)\n",
    "    \n",
    "    # Display sample text string as a bag of words.\n",
    "    crps = [dict_gensim.doc2bow(txt) for txt in txts]    \n",
    "    tfidf = models.TfidfModel(crps)\n",
    "        \n",
    "    # Compute  model for corpus\n",
    "    crps_tfidf = tfidf[crps]\n",
    "    \n",
    "    if model == 'lda':\n",
    "        mdl = models.LdaModel(corpus=crps_tfidf, id2word=dict_gensim, num_topics=topics)\n",
    "    else:\n",
    "        mdl = md.lsimodel.LsiModel(corpus=crps_tfidf, id2word=dict_gensim, num_topics=topics)\n",
    "      \n",
    "    return mdl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57b7faa2ece51e2a1659a3c50ef61fc2",
     "grade": true,
     "grade_id": "problem3_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lsi_gs=gensim_tm(tokens, 'lsi', 5)\n",
    "lda_gs=gensim_tm(tokens, 'lda', 5)\n",
    "assert_equal(lsi_gs.num_topics, 5)\n",
    "assert_equal(lda_gs.num_topics, 5)\n",
    "assert_equal(lsi_gs.num_terms, 6)\n",
    "assert_equal(lda_gs.num_terms, 6)\n",
    "lsi_gs2=gensim_tm(tokens, 'lsi', 8)\n",
    "lda_gs2=gensim_tm(tokens, 'lda', 8)\n",
    "assert_equal(lsi_gs2.num_topics, 8)\n",
    "assert_equal(lda_gs2.num_topics, 8)\n",
    "assert_equal(lsi_gs2.num_terms, 6)\n",
    "assert_equal(lda_gs2.num_terms, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "53f19563716eda28af258638f9ab6e1f",
     "grade": false,
     "grade_id": "cell-d5f3c676e2b896a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.494*\"mother\" + 0.441*\"good\" + 0.440*\"driving\" + 0.388*\"health\" + 0.346*\"brother\" + 0.313*\"brocolli\"'),\n",
       " (1,\n",
       "  '0.575*\"mother\" + -0.571*\"good\" + -0.409*\"brocolli\" + 0.253*\"driving\" + -0.244*\"health\" + 0.230*\"brother\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_gs.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd0c55a1d4684d2349b7126bad0c1a1e",
     "grade": false,
     "grade_id": "cell-773c7b206f2ea81c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.296*\"mother\" + 0.295*\"driving\" + 0.195*\"brother\" + 0.072*\"health\" + 0.072*\"good\" + 0.071*\"brocolli\"'),\n",
       " (3,\n",
       "  '0.384*\"good\" + 0.230*\"brocolli\" + 0.162*\"brother\" + 0.075*\"mother\" + 0.075*\"health\" + 0.075*\"driving\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_gs.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
