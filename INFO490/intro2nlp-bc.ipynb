{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Basic Concepts\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we build on the analytics concepts presented previously to dive more deeply into text analytics. Specifically, we will move beyond simple tokenization to leverage the semantic information contained in the ordering and arrangement of text data to gain new insights. We will start by exploring alternative tokenization techniques provided by the NLTK library before delving into part-of-speech tagging and named entity recognition. \n",
    "\n",
    "We begin by parsing a simple text document that contains a course description. First, we will employ a sentence tokenizer, before changing to word, whitespace, and  word/punctuation tokenizers.\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Tokenization](#Tokenization)\n",
    "\n",
    "[Collocations](#Collocations)\n",
    "\n",
    "[Tagging](#Tagging)\n",
    "\n",
    "- [Part of Speech Tagging](#Part-of-Speech-Tagging)\n",
    "\n",
    "- [Named-Entity-Recognition](#Named-Entity-Recognition)\n",
    "\n",
    "[Corpus](#Corpus)\n",
    "\n",
    "- [Penn Treebank](#Penn-Treebank)\n",
    "\n",
    "- [Brown Corpus](#Brown-Corpus)\n",
    "\n",
    "- [Linking Taggers](#Linking-Taggers)\n",
    "\n",
    "- [Tagged Text Extraction](#Tagged-Text-Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Before introducing more advanced concepts, we first demonstrate how to use the NLTK library to perform tokenization. To get started, we use a course description, which consists of a number of paragraphs. We first tokenize the document by sentence, before tokenizing on words. Finally, we introduce two special tokenizers: [`WhitespaceTokenizer`][wst] and [`WordPunctTokenizer`][wpt], which differ in how punctuation is treated. As shown in the third and fourth Code cells, the former ignored punctuation (treating it as another character, while the latter breaks punctuation out separately, producing clean words.\n",
    "\n",
    "-----\n",
    "\n",
    "[wst]: http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WhitespaceTokenizer\n",
    "[wpt]: http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 sentances in course description\n",
      "----------------------------------------\n",
      "Students will first learn how to perform more  statistical data exploration and constructing and  evaluating statistical models.\n"
     ]
    }
   ],
   "source": [
    "# As a text example, we use the following course description.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, ',\n",
    "               'online course. This course will introduce advanced ',\n",
    "               'data science concepts by building ',\n",
    "               'on the foundational concepts presented in the ',\n",
    "               'prerequisite course: Foundations of Data Science. ', \n",
    "               'Students will first learn how to perform more ',\n",
    "               'statistical data exploration and constructing and ',\n",
    "               'evaluating statistical models. Next, students will ',\n",
    "               'learn machine learning techniques including supervised ',\n",
    "               'and unsupervised learning, dimensional reduction, and ',\n",
    "               'cluster finding. An emphasis will be placed on the ',\n",
    "               'practical application of these techniques to ',\n",
    "               'high-dimensional numerical data, time series data, ',\n",
    "               'image data, and text data. Finally, students will ',\n",
    "               'learn to use relational databases and cloud computing ',\n",
    "               'software components such as Hadoop, Spark, and NoSQL ',\n",
    "               'data stores. Students must have access to a fairly ',\n",
    "               'modern computer, ideally that supports hardware ',\n",
    "               'virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors ',\n",
    "               'and graduate students in any discipline who have ',\n",
    "               'either taken a previous data science course or ',\n",
    "               'have received instructor permission.']\n",
    "\n",
    "text = \" \".join(info_course)\n",
    "\n",
    "# Tokenize and display results. \n",
    "from nltk import sent_tokenize\n",
    "snts = sent_tokenize(text)\n",
    "print(f'{len(snts)} sentances in course description')\n",
    "print(40*'-')\n",
    "# Also display one representative sentence\n",
    "print(snts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 words in course description\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science', ':', 'This', 'class', 'is', 'an',\n",
      "  'asynchronous', ',', 'online', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize by words, display results, and a representive section of words\n",
    "from nltk import word_tokenize\n",
    "wtks = word_tokenize(text)\n",
    "\n",
    "print(f'{len(wtks)} words in course description')\n",
    "print(40*'-')\n",
    "\n",
    "# Display the tokens\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, \n",
    "                          width=80, compact=True)\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 words in course description (WS Tokenizer)\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science:', 'This', 'class', 'is', 'an', 'asynchronous,',\n",
      "  'online', 'course.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WS Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 words in course description (WP Tokenizer)\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science', ':', 'This', 'class', 'is', 'an',\n",
      "  'asynchronous', ',', 'online', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WP Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Collocations\n",
    "\n",
    "We previously discussed using multiple, adjacent words, which is known as n-grams (e.g., bigrams or trigrams). We can also build [collocations][nc], where we use NLTK to grab n-grams, but now with the possibility of applying filters, such as a minimum frequency of occurrence. We can employ an association measure, such as the [pointwise mutual information][wpmi] (PMI), to compute the importance of a collocation. PMI quantifies the likelihood of two words occurring together in a document to their chance superposition (from their individual distribution in the document). Thus, a PMI close to one implies two words almost always occur together, while a PMI close to zero implies two words are nearly independent and rarely occur together.\n",
    "\n",
    "-----\n",
    "[nc]: http://www.nltk.org/howto/collocations.html\n",
    "[wpmi]: https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 bi-grams in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('An', 'emphasis'), ('an', 'asynchronous'), ('any', 'discipline'),\n",
      "  ('as', 'Hadoop'), ('be', 'placed'), ('by', 'building'), ('can', 'install'),\n",
      "  ('cloud', 'computing'), ('cluster', 'finding'), ('components', 'such')]\n",
      "--------------------------------------------------\n",
      "Best 10 bi-grams occuring more than once in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('Data', 'Science'), ('class', 'is'), ('This', 'class'), ('on', 'the'),\n",
      "  ('students', 'will'), ('will', 'learn'), ('.', 'Students'),\n",
      "  ('data', 'science'), ('.', 'This'), (',', 'students')]\n"
     ]
    }
   ],
   "source": [
    "top_bgs = 10\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wtks)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "\n",
    "print(f'Best {top_bgs} bi-grams in course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(bgs)\n",
    "\n",
    "print(50*'-')\n",
    "print(f'Best {top_bgs} bi-grams occuring more than once in course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "pp.pprint(bgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 tri-grams in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('any', 'discipline', 'who'), ('components', 'such', 'as'),\n",
      "  ('fairly', 'modern', 'computer'), ('ideally', 'that', 'supports'),\n",
      "  ('received', 'instructor', 'permission'), ('such', 'as', 'Hadoop'),\n",
      "  ('supports', 'hardware', 'virtualization'), ('that', 'supports', 'hardware'),\n",
      "  ('they', 'can', 'install'), ('use', 'relational', 'databases')]\n",
      "--------------------------------------------------\n",
      "Best 10 tri-grams occuring more than once in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('This', 'class', 'is'), ('students', 'will', 'learn'),\n",
      "  (',', 'students', 'will')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(wtks)\n",
    "tgs = finder.nbest(trigram_measures.pmi, top_bgs)\n",
    "\n",
    "print(f'Best {top_bgs} tri-grams in course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(tgs)\n",
    "\n",
    "print(50*'-')\n",
    "print(f'Best {top_bgs} tri-grams occuring more than once in course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "tgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "pp.pprint(tgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Tagging\n",
    "\n",
    "The simplest approach to text analysis is the bag-of-words model, where we simply identify the words (or tokens) present in a set of documents. In order to move beyond this model, we need to include additional information with each word. For example, the word _duck_ can mean the bird or it can mean the action. More generally, this concept when applied to multiple words is known as a [garden path sentences][wgps]. \n",
    "\n",
    "In the bag of word model, the difference between these two meanings (of the word _duck_) is lost. By associating information about the context or the grammatical nature of a word, however, these different use cases can be distinguished. The mechanism by which this is done is known as tagging. A tag can be used to identify the grammatical nature of a word, like _noun_ or _verb_, or it can be other information, including associations with other words in the text. In the following Code cells, we first introduce a _DefaultTagger_, which associates a tag of our choosing with words. Afterwards, we use the NLTK built-in Part of Speech (POS) and Named Entity Recognition (NER) taggers.\n",
    "\n",
    "-----\n",
    "[wgps]: https://en.wikipedia.org/wiki/Garden_path_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('Advanced', 'INFO'), ('Data', 'INFO'), ('Science', 'INFO'), (':', 'INFO'),\n",
      "  ('This', 'INFO'), ('class', 'INFO'), ('is', 'INFO'), ('an', 'INFO'),\n",
      "  ('asynchronous', 'INFO'), (',', 'INFO'), ('online', 'INFO'),\n",
      "  ('course', 'INFO'), ('.', 'INFO')]\n"
     ]
    }
   ],
   "source": [
    "a_tag = 'INFO'\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "default_tagger = DefaultTagger(a_tag)\n",
    "tgs = default_tagger.tag(wtks)\n",
    "\n",
    "print('Tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Part of Speech Tagging\n",
    "\n",
    "Part of speech (PoS) simply refers to the grammatical properties of a word. While this might seem simple, given the diversity of languages (and even variations within a single language), this topic quickly becomes quite substantial. As a result, there are a number of possible approaches. In the next two Code cells, we first demonstrate a simple PoS that labels only basic text components such as _Noun_, _Verb_, or _Adjective_, before moving to a more complex PoS that labels a wider range of text components, which can also establish grammatical relationships between multiple words.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer/Univesal Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NOUN'), ('Data', 'NOUN'), ('Science', 'NOUN'), (':', '.'),\n",
      "  ('This', 'DET'), ('class', 'NOUN'), ('is', 'VERB'), ('an', 'DET'),\n",
      "  ('asynchronous', 'ADJ'), (',', '.'), ('online', 'ADJ'), ('course', 'NOUN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "ptgs = pos_tag(wtks, tagset='universal')\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Univesal Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "pp.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "PoS tags can be much more complex, as shown in the following Code cell. The specific tags depend on the selected tagset, by default NLTK now uses a [_PerceptronTagger_][pt], which quickly generates a set of tagged grammatical constructs.\n",
    "\n",
    "----\n",
    "[pt]: http://spacy.io/blog/part-of-speech-POS-tagger-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer/Default Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'), ('Data', 'NNP'), ('Science', 'NN'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'), (',', ','), ('online', 'JJ'), ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "ptgs = pos_tag(wtks)\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Default Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "pp.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) classifies (or recognizes) chunks of text that refer to pre-defined categories (or named entities). These chunks can be one or more words, and the categories can be names of people, organizations, locations, or other types of entities. For example, in the following sentence:\n",
    "\n",
    "> Edward is a graduate student enrolled at the University of Illinois.\n",
    "\n",
    "_Edward_ is a person and _University of Illinois_ is an organization. NLTK can be used to identify named entities, generally following a part of speech tagging (to clarify different uses of words that otherwise might cause confusion). In the following Code cell, we demonstrate NER by using NLTK to identify named entities in the course description text.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "NER tagged course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ Tree('PERSON', [('Advanced', 'NNP')]),\n",
      "  Tree('ORGANIZATION', [('Data', 'NNP'), ('Science', 'NN')]), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'), (',', ','), ('online', 'JJ'), ('course', 'NN'),\n",
      "  ('.', '.'), ('This', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "nrcs = ne_chunk(pos_tag(wtks))\n",
    "\n",
    "print(50*'-')\n",
    "print('NER tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(nrcs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Corpus\n",
    "\n",
    "A corpus is simply a collection of documents. In the case of Natural Language Processing, however, a corpus can include additional information for both part of speech tagging and named entity recognition. The NLTK library includes several corpuses, including the Penn Treebank, Brown, and Wordnet. In the rest of this notebook, we introduce the first two corpuses; the Wordnet corpus is introduced in in the notebook discusing semantic analysis.\n",
    "\n",
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "###  Penn Treebank\n",
    "\n",
    "The [Penn Treebank project][ptbp] is an effort to annotate text, into a linguistic structure. This structure is generally in the form of a [tree][wt], within which the different components of a sentence are organized. This process includes a [part of speech tagging][ptpos]. We demonstrate the use of the Penn Treebank with NLTK in the next few Code cells, where we tokenize text by using a Penn Treebank standard sentence and word tokenizer, and tagged sentence and word tokenizers. Finally, we introduce the `UnigramTagger`, which can be trained on a given corpus to tokenize and tag unigrams in a new document (or set of documents).\n",
    "\n",
    "-----\n",
    "[ptbp]: https://www.cis.upenn.edu/~treebank/\n",
    "[ptpos]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "[wt]: https://en.wikipedia.org/wiki/Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged text.\n",
      "--------------------------------------------------------------------------------\n",
      "Words:     [ 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "  'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "--------------------------------------------------------------------------------\n",
      "Setnences: [ 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "  'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "--------------------------------------------------------------------------------\n",
      "Tagged Words: \n",
      "[ ('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'),\n",
      "  ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'),\n",
      "  ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'),\n",
      "  ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'),\n",
      "  ('.', '.')]\n",
      "--------------------------------------------------------------------------------\n",
      "Tagged Sentances: \n",
      "[ ('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'),\n",
      "  ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'),\n",
      "  ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'),\n",
      "  ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'),\n",
      "  ('.', '.')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "print('Penn Treebank tagged text.')\n",
    "print(80*'-')\n",
    "\n",
    "print('Words:     ', end='')\n",
    "pp.pprint(treebank.words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Setnences: ', end='')\n",
    "pp.pprint(treebank.sents()[0])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Words: ')\n",
    "pp.pprint(treebank.tagged_words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Sentances: ')\n",
    "pp.pprint(treebank.tagged_sents()[0])\n",
    "print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "pt_tagger = UnigramTagger(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'), ('Data', 'NNP'), ('Science', 'NN'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('an', 'DT'),\n",
      "  ('asynchronous', None), (',', ','), ('online', None), ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pt_tgs = pt_tagger.tag(wtks)\n",
    "\n",
    "print('Penn Treebank tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "pp.pprint(pt_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Brown Corpus\n",
    "\n",
    "The [Brown Corpus][wbc] has over one million tagged words, and was originally published in 1967. The corpus itself is composed of 500 samples, spread over fifteen different genres, of English-language text compiled from works published in 1961. NLTK provides the Brown Corpus, which can be used to tag new documents, as shown below.\n",
    "\n",
    "----\n",
    "[wbc]: https://en.wikipedia.org/wiki/Brown_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "b_tagger = UnigramTagger(brown.tagged_sents(brown.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'JJ-TL'), ('Data', 'NNS-TL'), ('Science', 'NN-TL'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'BEZ'), ('an', 'AT'),\n",
      "  ('asynchronous', None), (',', ','), ('online', None), ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "pp.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Linking Taggers\n",
    "\n",
    "\n",
    "In the previous examples, certain words were left untagged or tagged with `None` (such as _online_ or _asynchronous_). Since language evolves over time, an older corpus might miss words, or they may simply be incomplete. To handle these cases, NLTK enables taggers to be linked. Thus a general tagger can be applied, such as the Brown Corpus, after which a second tagger can be applied to increase the number of words tagged. This is a common application area for a _DefaultTagger_, which can be used to assign a specific tag to any element missed by another tagger. We demonstrate this concept below, by linking the Brown Corpus tagger with our earlier Default tagger.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged course description (WP Tokenizer/Linked Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'JJ-TL'), ('Data', 'NNS-TL'), ('Science', 'NN-TL'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'BEZ'), ('an', 'AT'),\n",
      "  ('asynchronous', 'INFO'), (',', ','), ('online', 'INFO'), ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# We can link taggers\n",
    "\n",
    "b_tagger._taggers = [b_tagger, default_tagger]\n",
    "\n",
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer/Linked Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "pp.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Tagged Text Extraction\n",
    "\n",
    "For some text analysis projects, we might want to restrict words (or tokens) to specific tags. For example, we might prefer to only use _Nouns_, _Primary Verbs_, or _Adjectives_ for text classification. To extract only terms that meet these conditions, we can tag the text, and apply a regular expression to the tagged tokens, as shown in the following Code cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'), ('Data', 'NNP'), ('Science', 'NN'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'), (',', ','), ('online', 'JJ'), ('course', 'NN'),\n",
      "  ('.', '.')]\n",
      "------------------------------------------------------------\n",
      "POS tagged course description (WP Tokenizer/RegEx applied)\n",
      "------------------------------------------------------------\n",
      "['Advanced', 'Data', 'Science', 'class', 'asynchronous', 'online', 'course']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# NN matchs NN|NNS|NNP|NNPS\n",
    "rgxs = re.compile(r\"(JJ|NN|VBN|VBG)\")\n",
    "\n",
    "ptgs = pos_tag(wtks)\n",
    "trms = [tkn[0] for tkn in ptgs if re.match(rgxs, tkn[1])]\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "pp.pprint(ptgs[:13])\n",
    "print(60*'-')\n",
    "print('POS tagged course description (WP Tokenizer/RegEx applied)')\n",
    "print(60*'-')\n",
    "pp.pprint(trms[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we introduced several basic NLP concepts, including tagging, Part of Speech, and Named Entity Recognition. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change from a Unigram tagger to a Bigram Tagger. How do you results change?\n",
    "2. Replace the initial text with a longer document (you can use a text from within NLTK or a freely available text from _Project Gutenberg_). Apply more restrictive filters (i.e., higher frequencies) to the bigrams and trigrams, do your results make sense?\n",
    "3. Try using regular expressions to restrict tokens in the NLTK movie review data set to Nouns, Verbs, Adjectives, and Adverbs. Use these tokens to perform Sentiment Analysis on these movie review data. Are the results better or worse than with all words?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia article on [Treebanks][wtb]\n",
    "2. The [Penn Treebank][ptb] project\n",
    "3. Wikipedia article on [Garden Path Sentences][wgps]\n",
    "4. Sections 1, 2, and 4-7 from Chapter 5 of the free [NLTK version 3.0][nltk3-5] book\n",
    "4. Sections 5 and 6 from Chapter 7 of the free [NLTK version 3.0][nltk3-7] book\n",
    "1. [Spacy][sp], a new natural language processing library.\n",
    "2. Blog entry on using spacy to [mark adverbs][bma]\n",
    "3. Blog entry on [Named Entity Recognition][yner]\n",
    "\n",
    "-----\n",
    "\n",
    "[wtb]: https://en.wikipedia.org/wiki/Treebank\n",
    "[wgps]: https://en.wikipedia.org/wiki/Garden_path_sentence\n",
    "\n",
    "[yner]: http://blog.yhat.com/posts/named-entities-in-law-and-order-using-nlp.html\n",
    "\n",
    "[nltk3-5]: http://www.nltk.org/book/ch05.html\n",
    "[nltk3-7]: http://www.nltk.org/book/ch07.html\n",
    "\n",
    "[bma]: https://spacy.io/tutorials/mark-adverbs\n",
    "[sp]: https://web.archive.org/web/20151103165613/http://spacy.io/tutorials/mark-adverbs/\n",
    "[ptb]: https://web.archive.org/web/20160829023945/http://www.cis.upenn.edu/~treebank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
