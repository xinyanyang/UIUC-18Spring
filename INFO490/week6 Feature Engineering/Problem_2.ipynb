{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f0a5efa111262c68f81374d197ee20d",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 6 Problem 2\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says YOUR CODE HERE. Do not write your answer in anywhere else other than where it says YOUR CODE HERE. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select Kernel, and restart the kernel and run all cells (Restart & Run all).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select File → Save and CheckPoint)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to Dashboard → Assignments and click the Submit button. Your work is not submitted until you click Submit.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "7. If your code does not pass the unit tests, it will not pass the autograder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b6c2d838d0d7f522527e85fc9e2f016",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, February 26, 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "75f562ffd21a143ff1d527aa406d09ac",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "from pandas.util.testing import assert_frame_equal, assert_index_equal\n",
    "from nose.tools import assert_false, assert_equal, assert_almost_equal, assert_true, assert_in, assert_is_not\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set global figure properties\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'axes.titlesize' : 20,\n",
    "                     'axes.labelsize' : 18,\n",
    "                     'legend.fontsize': 16})\n",
    "\n",
    "# Set default seaborn plotting style\n",
    "sns.set_style('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c48650523affd26809097deb42ef784b",
     "grade": false,
     "grade_id": "dataset",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Breast Cancer Dataset\n",
    "\n",
    "We will be using the built-in dataset about breast cancer and the respective information on indivudal breast cancer cases. This dataset has 569 samples and a dimensionality size of 30. We will be using only the 1st 10 features in order to create a Gradient Boosting model that will predict whether the individual case is either malignant (harmful) or benign (non-harmful).\n",
    "\n",
    "The following code below imports the dataset as a pandas dataframe. It also concatenates a column called classification which contains whether the record was determined to be a malignant or benign tumor. Note: In this dataset, a malignant tumor has a value of 0 and a benign tumor has a value of 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "372e758a25922441f648fa22eb9b01e7",
     "grade": false,
     "grade_id": "load_data",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    212\n",
      "1    357\n",
      "Name: target, dtype: int64\n",
      "Number of features: 31\n"
     ]
    }
   ],
   "source": [
    "# Load in the dataset as a Pandas DataFrame\n",
    "data = load_breast_cancer()\n",
    "cancer_data = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "cancer_data['target'] = data.target\n",
    "# View the label distribution\n",
    "print(cancer_data.target.value_counts(ascending=True))\n",
    "\n",
    "features = cancer_data[cancer_data.columns]\n",
    "labels = cancer_data.target\n",
    "# Count the number of features\n",
    "print(\"Number of features:\", len(features.columns))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0bebea4b75b05c420d5c01f106c081e0",
     "grade": false,
     "grade_id": "problem1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "We can see that the above Breast Cancer Dataset has 30 independent features. In this problem, we will perform the PCA on this dataset so as to reduce the number of features. \n",
    "Performing PCA on unscaled variables will lead to insanely large loadings for variables with high variance. In turn, this will lead to dependence of a principal component on the variable with high variance. This is undesirable.\n",
    "\n",
    "Create a function which will perform PCA on the features passed as parameters. A scale parameter is used in the function which will specify whether to perform scaling or not. Function should return the length of the transformed feature set(after PCA) and the number of new variables explaining nearly **'n'** percentage of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7fb5d1e3683d01073654a2d9b32818df",
     "grade": false,
     "grade_id": "problem1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def pca(features, n , scale):\n",
    "    '''\n",
    "    Fit PCA model on features and return the transformed dataset along with the number of variables\n",
    "    (after PCA) explaining n percentage variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features : variables on which PCA is to be applied\n",
    "    n : percentage cutoff of cumulative variance explained by variables after PCA \n",
    "    scale : A boolean value(if true, then transform variables using StandardScaler else do nothing)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 2 containing(in respective order) :\n",
    "    Number of variables explaining variance less than or equal to n(just the length), \n",
    "    The transformed data after PCA using fit_transform.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    Find the cumulative variance explained and then take take the length of (cumulative variance<=n).\n",
    "    Remember to transform the dataset after PCA(If scaling is True, there will be 2 transformations)\n",
    "    '''\n",
    "    \n",
    "    # Principal Component Analysis\n",
    "    pca = PCA()\n",
    "    #standard scaler\n",
    "    ss = StandardScaler()\n",
    "    \n",
    "    if scale == True:\n",
    "        features = ss.fit_transform(features)\n",
    "        \n",
    "    # Fit model to the data\n",
    "    features = pca.fit_transform(features)\n",
    "    \n",
    "    vars = pca.explained_variance_ratio_\n",
    "    \n",
    "    sum_var = 0\n",
    "    i = 0\n",
    "    while (sum_var < n):\n",
    "        sum_var = sum_var + vars[i]\n",
    "        i = i + 1\n",
    "    \n",
    "    return i-1, features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1db8c558f5f5c9ec11b5ca95a8da4ed6",
     "grade": true,
     "grade_id": "problem1_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "length, features_reduced = pca(features, 0.9, True)\n",
    "assert_equal(length, 6)\n",
    "assert_equal(isinstance(features_reduced, np.ndarray), True)\n",
    "assert_almost_equal(features_reduced[0][0], 9.22577, 3)\n",
    "length1, features_reduced1 = pca(features, 0.995, False)\n",
    "assert_equal(length1, 1)\n",
    "assert_almost_equal(features_reduced1[0][0], 1160.1427, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66c5d25fce3e3ee6bbb1416076c76679",
     "grade": false,
     "grade_id": "problem2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "Create a test-train split where test_size=0.3 and random_state=23 after subsetting the transformed feature set based on the length parameter to transform our new feature space to 'length' dimensions.\n",
    "\n",
    "In the code cell below, we will create a KNeighborsClassifier to classify the tumor as malignant or benign. We will use 'skf' for purpose of cross validation. We will try different values of **k_vals** to build the model using Grid Search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f2ec2e3159c7ac2705807b79509d0a7d",
     "grade": false,
     "grade_id": "problem2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def knn(k_vals, length, features_reduced, labels):\n",
    "    '''\n",
    "    Subset the features based on the length parameter passed in the function i.e. the new feature space should \n",
    "    be only of 'length' dimensions.\n",
    "\n",
    "    Perform a test train split with this reduced feature set and lables with following parameters:\n",
    "    test_size=0.3, random_state=23\n",
    "\n",
    "    Create a Grid Search cross validator(cv=skf) for KNeighborsClassifier where param_grid will be a dictionary \n",
    "    containing n_neighbors as hyperparameter and k_vals as values. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k_vals : range of nearest neighbors value passed as a numpy array\n",
    "    length : number of dimensions that the feature set(transformed after PCA) should be reduced to.\n",
    "    features_reduced : the reduced features set obtained after PCA transformation.\n",
    "    labels : the original labels from the data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Training features as a multi dimensional numpy array (contains 70% of the features)\n",
    "    Testing features as a multi dimensional numpy array (contains 30% of the features)\n",
    "    Training labels as pandas series (contains 70% of the labels)\n",
    "    Testing labels as pandas series (contains 30% of the labels)\n",
    "    Grid search cross validator instance which has the knn as estimator, paramater grid containing neighbor values \n",
    "    and cross-validation = 'skf' as parameters.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    features_reduced is a 2D numpy array containing 569 observations and 31 features. \n",
    "    Make sure to subset this properly(i.e. don't subset the observations instead of features)  \n",
    "    '''\n",
    "    #subset the features\n",
    "    data_reduced = features_reduced[:, :length]\n",
    "    \n",
    "    #train-test-split\n",
    "    X_train, X_test, y_train, y_test \\\n",
    "       = train_test_split(data_reduced, labels, test_size = 0.3, random_state = 23)\n",
    "    \n",
    "    #create an estimator \n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    # Create a dictionary of hyperparameters and values\n",
    "    params = dict(n_neighbors=k_vals)\n",
    "    \n",
    "    #create a grid search cross validator\n",
    "    gse = GridSearchCV(estimator = knn, param_grid = params, cv = skf)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, gse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66d96982ca476b5353de30c57c4d877f",
     "grade": true,
     "grade_id": "problem2_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "k_vals = np.arange(1,101,2)\n",
    "f_train, f_test, l_train, l_test, gknn = knn(k_vals, length, features_reduced, labels)\n",
    "gknn.fit(f_train, l_train)\n",
    "bknn=gknn.best_estimator_\n",
    "assert_equal(type(f_train), type(np.ndarray(0)))\n",
    "assert_equal(type(f_test), type(np.ndarray(0)))\n",
    "assert_equal(type(l_train), type(pd.core.series.Series()))\n",
    "assert_equal(type(l_test), type(pd.core.series.Series()))\n",
    "assert_equal(isinstance(gknn, GridSearchCV), True)\n",
    "assert_equal(isinstance(bknn, KNeighborsClassifier), True)\n",
    "assert_almost_equal(0.972, gknn.best_score_, 2)\n",
    "assert_almost_equal(0.982, gknn.score(f_test, l_test), 2)\n",
    "assert_equal(len(f_train[0,:]), 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7632ae69ba81b626ace3386fdce8c650",
     "grade": false,
     "grade_id": "problem3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "In this problem, we will create a function which will create an instance for one of the following methods : PCA, NMF, FactorAnalysis, FastICA and MiniBatchSparsePCA. Use **n_components** as the hyperparameter. Your function should return the method instance along with the transformed dataset.\n",
    "\n",
    "***Hint***: Use fit_transform to transform the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a946dc38c77b86913168c3871eef1141",
     "grade": false,
     "grade_id": "problem3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Reduction(method, features, n):\n",
    "    '''\n",
    "    This function will use method parameter to create an instance for one of the following methods:\n",
    "    PCA, NMF, FactorAnalysis, FastICA and MiniBatchSparsePCA and return the transformed dataset along with method used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method : one of either PCA, NMF, FactorAnalysis, FastICA or MiniBatchSparsePCA\n",
    "    features : feature set that should be transformed based on the method used.\n",
    "    n : n_components \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 2 containing the instance of the method created and the transformed features based on \n",
    "    model used respectively\n",
    "    '''\n",
    "\n",
    "    if method == 'PCA' :\n",
    "        classifier = PCA\n",
    "    elif method == 'NMF':\n",
    "        classifier = NMF\n",
    "    elif method == 'FactorAnalysis':\n",
    "        classifier = FactorAnalysis\n",
    "    elif method == 'FastICA':\n",
    "        classifier = FastICA\n",
    "    else:\n",
    "        classifier = MiniBatchSparsePCA\n",
    "        \n",
    "    model = classifier(n_components = n)\n",
    "    features = model.fit_transform(features)\n",
    "    \n",
    "    return model, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ba20b7d7f6f758de62fc4bd9d51f411d",
     "grade": true,
     "grade_id": "problem3_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pca_model, X1 = Reduction(\"PCA\", features, 3)\n",
    "nmf_model, X2 = Reduction(\"NMF\", features, 3)\n",
    "fa_model, X3 = Reduction(\"FactorAnalysis\", features, 3)\n",
    "fica_model, X4 = Reduction(\"FastICA\", features, 3)\n",
    "sp_model, X5 = Reduction(\"MiniBatchSparsePCA\", features, 3)\n",
    "\n",
    "assert_equal(type(pca_model), PCA)\n",
    "assert_equal(type(nmf_model), NMF)\n",
    "assert_equal(type(fa_model), FactorAnalysis)\n",
    "assert_equal(type(fica_model), FastICA)\n",
    "assert_equal(type(sp_model), MiniBatchSparsePCA)\n",
    "assert_almost_equal(X1[0][0], 1160.14274385, 0)\n",
    "\n",
    "f_train, f_test, l_train, l_test \\\n",
    "    = train_test_split(X2, labels, \n",
    "                        test_size=0.3, random_state=23)\n",
    "model = DecisionTreeClassifier(random_state=40, max_depth = 5)\n",
    "model.fit(f_train, l_train)\n",
    "predicted = model.predict(f_test)\n",
    "\n",
    "assert_almost_equal(X2[0][0], 12.84946978, 0)\n",
    "assert_almost_equal(0.9532, metrics.accuracy_score(l_test, predicted), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
